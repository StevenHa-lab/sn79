# SPDX-FileCopyrightText: 2025 Rayleigh Research <to@rayleigh.re>
# SPDX-License-Identifier: MIT
# The MIT License (MIT)
# Copyright © 2023 Yuma Rao
# Copyright © 2025 Rayleigh Research

# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated
# documentation files (the “Software”), to deal in the Software without restriction, including without limitation
# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software,
# and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

# The above copyright notice and this permission notice shall be included in all copies or substantial portions of
# the Software.

# THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO
# THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
# OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
# DEALINGS IN THE SOFTWARE.

import os
import time
import bittensor as bt
import uvloop
import asyncio
import aiohttp
import posix_ipc
import struct
import traceback
import pickle
from typing import List

from taos.im.neurons.validator import Validator
from taos.im.protocol import FinanceAgentResponse, FinanceEventNotification, MarketSimulationStateUpdate
from taos.im.protocol.instructions import *
from taos.im.validator.reward import set_delays
from taos.im.utils import duration_from_timestamp

asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())

class DendriteManager:
    @staticmethod
    def configure_session(validator):
        """
        Ensures the validator's dendrite client session is properly configured.

        Creates a new aiohttp session if none exists or the previous one is closed.
        Reuses an existing session if available.

        Args:
            validator (Validator): The validator whose dendrite session should be configured.
        Returns:
            None
        """
        if not validator.dendrite._session or validator.dendrite._session.closed:
            connector = aiohttp.TCPConnector(
                ssl=False,
                limit=0,
                limit_per_host=0,
                ttl_dns_cache=300,
                enable_cleanup_closed=True,
            )

            timeout = aiohttp.ClientTimeout(
                total=validator.config.neuron.timeout,
                connect=1.0,
                sock_read=None,
                sock_connect=1.0,
            )

            validator.dendrite._session = aiohttp.ClientSession(
                connector=connector,
                timeout=timeout,
                skip_auto_headers={'User-Agent'},
            )
            bt.logging.debug("Created new aiohttp session")
        else:
            bt.logging.debug("Reusing existing aiohttp session")

def update_stats(self : Validator, synapses : dict[int, MarketSimulationStateUpdate]) -> None:
    """
    Updates miner request statistics maintained and published by the validator.

    Args:
        self (Validator): The intelligent markets simulation validator.
        synapses (dict[int, MarketSimulationStateUpdate]): Mapping of miner UIDs to their corresponding synapse updates.

    Returns:
        None
    """
    for uid, synapse in synapses.items():
        self.miner_stats[uid]['requests'] += 1
        if synapse.is_timeout:
            self.miner_stats[uid]['timeouts'] += 1
        elif synapse.is_failure or synapse.response is None:
            self.miner_stats[uid]['failures'] += 1
        elif synapse.is_blacklist:
            self.miner_stats[uid]['rejections'] += 1
        elif synapse.dendrite.process_time:
            self.miner_stats[uid]['call_time'].append(synapse.dendrite.process_time)

async def forward(self, synapse: MarketSimulationStateUpdate) -> List[FinanceAgentResponse]:
    """
    Forwards a market simulation state update to miner agents via POSIX IPC.

    Packages request data, sends it to the external query service, receives agent responses,
    updates miner statistics, and applies delay scoring.

    Args:
        self (Validator): The intelligent markets simulation validator.
        synapse (MarketSimulationStateUpdate): The simulation state data to forward to miner agents.

    Returns:
        List[FinanceAgentResponse]: List of agent responses generated by miners.
    """
    responses = []

    if self.deregistered_uids != []:
        response = FinanceAgentResponse(agent_id=self.uid)
        response.reset_agents(agent_ids=self.deregistered_uids)
        responses.append(response)

    bt.logging.info("Querying Miners...")

    if self.query_process.poll() is not None:
        bt.logging.error(f"Query service died with exit code {self.query_process.returncode}")
        bt.logging.error("Attempting to restart query service...")
        self._start_query_service()
        if self.query_process.poll() is not None:
            self.pagerduty_alert("Failed to restart query service")
            return responses
    try:
        session_start = time.time()
        bt.logging.debug(f"Session configured ({time.time() - session_start:.4f}s)")

        data_start = time.time()
        request_data = {
            'books': synapse.books,
            'accounts': synapse.accounts,
            'notices': synapse.notices,
            'config': synapse.config.model_dump(mode='json'),
            'version': synapse.version,
            'timestamp': synapse.timestamp,
            'metagraph_axons': [
                {
                    'hotkey': axon.hotkey,
                    'coldkey': axon.coldkey,
                    'ip': axon.ip,
                    'port': axon.port,
                    'ip_type': axon.ip_type,
                    'protocol': axon.protocol,
                }
                for axon in self.metagraph.axons
            ],
            'deregistered_uids': list(self.deregistered_uids),
            'uid': self.uid,
            'miner_wealth': self.simulation.miner_wealth,
            'volume_decimals': self.simulation.volumeDecimals,
            'book_count': self.simulation.book_count,
            'volume_sums': {uid: dict(books) for uid, books in self.volume_sums.items()},
            'capital_turnover_cap': self.config.scoring.activity.capital_turnover_cap,
            'max_instructions_per_book': self.config.scoring.max_instructions_per_book,
        }
        bt.logging.info(f"Request Data Prepared ({time.time()-data_start:.4f}s).")

        if self.should_block_queries():
            last_log_time = time.time()
            while self._pending_reward_tasks > 0:
                await asyncio.sleep(0.1)
                current_time = time.time()
                if current_time - last_log_time >= 1.0:
                    bt.logging.warning(f"Waiting for rewarding to catch up before querying ({self._pending_reward_tasks} tasks pending)...")
                    last_log_time = current_time
        bt.logging.info(f"Rewarding up to date, proceeding with query!")

        self.querying = True
        query_start = time.time()
        try:
            try:
                await asyncio.wait_for(
                    asyncio.get_event_loop().run_in_executor(
                        self.query_ipc_executor,
                        lambda: self.response_queue.receive(timeout=0.001)
                    ),
                    timeout=0.1
                )
                bt.logging.warning("Drained stale message from query response queue")
            except (posix_ipc.BusyError, asyncio.TimeoutError):
                pass
            bt.logging.info(f"Drained Query Response Queue ({time.time()-query_start:.4f}s).")

            write_start = time.time()
            data_bytes = pickle.dumps(request_data, protocol=5)
            bt.logging.info(f"Query request data size: {len(data_bytes) / 1024 / 1024:.2f} MB")
            
            self.request_mem.seek(0)
            self.request_mem.write(struct.pack('Q', len(data_bytes)))
            self.request_mem.write(data_bytes)
            bt.logging.info(f"Wrote query request data ({time.time()-write_start:.4f}s).")

            self.response_mem.seek(0)
            self.response_mem.write(struct.pack('Q', 0))

            receive_start = time.time()
            self.request_queue.send(b'query')
            bt.logging.info(f"Sent query request, waiting for notification...")

            max_wait = self.config.neuron.global_query_timeout + 10
            
            try:
                loop = asyncio.get_event_loop()
                future = asyncio.Future()
                
                def pipe_readable():
                    """Callback when pipe becomes readable"""
                    loop.remove_reader(self.query_notify_read)
                    try:
                        data = os.read(self.query_notify_read, 1)
                        future.set_result(data)
                    except Exception as e:
                        future.set_exception(e)
                loop.add_reader(self.query_notify_read, pipe_readable)
                try:
                    notify_result = await asyncio.wait_for(future, timeout=max_wait)
                except asyncio.TimeoutError:
                    loop.remove_reader(self.query_notify_read)
                    raise
                pipe_wait_time = time.time() - receive_start
                if notify_result == b'1':
                    bt.logging.info(f"Received query completion notification ({pipe_wait_time:.4f}s)")
                elif notify_result == b'R':
                    bt.logging.debug("Ignoring stray ready signal")
                    return await forward(self, synapse)
                else:
                    bt.logging.warning(f"Unexpected notification: {notify_result}")
                    return responses
            except asyncio.TimeoutError:
                self.pagerduty_alert(f"Query service notification timeout after {max_wait}s")
                return responses
            except Exception as e:
                self.pagerduty_alert(f"Error waiting for notification: {e}")
                return responses
            
            read_start = time.time()
            self.response_mem.seek(0)
            size_bytes = self.response_mem.read(8)
            data_size = struct.unpack('Q', size_bytes)[0]
            
            if data_size == 0:
                self.pagerduty_alert("Query service signaled ready but response size is 0")
                return responses
            
            result_bytes = self.response_mem.read(data_size)
            
            if len(result_bytes) != data_size:
                self.pagerduty_alert(f"Incomplete read: got {len(result_bytes)}, expected {data_size}")
                return responses
            
            try:
                result = pickle.loads(result_bytes)
                bt.logging.info(f"Read query response data ({time.time()-read_start:.4f}s)")
            except (pickle.UnpicklingError, UnicodeDecodeError) as e:
                self.pagerduty_alert(f"Failed to unpickle query response: {e}")
                return responses

        except posix_ipc.BusyError:
            self.pagerduty_alert(f"Query service response timeout")
            return responses
        except Exception as e:
            self.pagerduty_alert(f"Error communicating with query service: {e}", details={"traceback" : traceback.format_exc()})
            return responses

        if not result['success']:
            self.pagerduty_alert(f"Query service error: {result.get('error')}")
            if 'traceback' in result:
                bt.logging.error(f"Traceback: {result['traceback']}")
            return responses

        synapse_responses = result['responses']
        
        bt.logging.info(f"Query Completed ({time.time()-query_start:.4f}s).")
        
        start = time.time()
        update_stats(self, synapse_responses)
        bt.logging.info(f"Updated Stats ({time.time()-start:.4f}s).")

        start = time.time()
        responses.extend(set_delays(self, synapse_responses))
        bt.logging.info(f"Set Delays ({time.time()-start:.4f}s).")

        bt.logging.trace(f"Responses: {responses}")
        bt.logging.info(f"Received {result['validation_stats']['total_responses']} valid responses containing {result['validation_stats']['total_instructions']} instructions at {duration_from_timestamp(synapse.timestamp)} "
                    f"({result['validation_stats']['success']} SUCCESS | {result['validation_stats']['timeouts']} TIMEOUTS | {result['validation_stats']['failures']} FAILURES).")
        return responses
    finally:
        self.querying = False
        bt.logging.debug("Query flag cleared")

    return responses

async def notify(self : Validator, notices : List[FinanceEventNotification]) -> None:
    """
    Forwards finance event notifications to the appropriate miner agents.

    Uses the validator's dendrite to dispatch each event to the miner responsible
    for the associated agent, or to all miners if the event is broadcast.

    Args:
        self (Validator): The intelligent markets simulation validator.
        notices (List[FinanceEventNotification]): List of event notifications to dispatch.

    Returns:
        None
    """
    responses = []
    for notice in notices:
        axons = [self.metagraph.axons[notice.event.agentId]] if notice.event.agentId else self.metagraph.axons
        responses.extend(await self.dendrite(
            axons=axons,
            synapse=notice,
            timeout=1
        ))
    for response in responses:
        if response and response.acknowledged:
            bt.logging.info(f"{response[0].type} EventNotification Acknowledged by {response[0].axon.hotkey}")